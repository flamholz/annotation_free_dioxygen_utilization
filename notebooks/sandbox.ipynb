{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aerobot.io import FEATURE_TYPES, DATA_PATH\n",
    "from aerobot.dataset import dataset_load_feature_order, dataset_load_training_validation\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_jablonska(feature_type:str, path:str=os.path.join(DATA_PATH, 'jablonska/'), ):\n",
    "    return pd.read_csv(os.path.join(path, f'jablonska_{feature_type}.csv'), index_col=0)\n",
    "    \n",
    "\n",
    "def load_data_madin(feature_type:str, path:str=os.path.join(DATA_PATH, 'madin/madin.h5')):\n",
    "    # Create a dictionary mapping each feature type to a key in the HD5 file.\n",
    "    key_map = {f:f for f in FEATURE_TYPES} # Most keys are the same as the feature type names.\n",
    "    key_map.update({'embedding.genome':'WGE', 'embedding.geneset.oxygen':'OGSE', 'metadata':'AF'})\n",
    "    key_map.update({'labels':'labels'})\n",
    "    return pd.read_hdf(path, key=key_map[feature_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curious about how much the annotated KO groups in the Jablonska and Madin datasets overlapped. I am running into a bug due to the fact that one (or both) of the datasets have no proteins annotated with some of the KO groups in the list of terminal oxidase KO groups. I am trying to figure out if it is better to fill in zeros or drop the missing KO groups.\n",
    "\n",
    "If both datasets are missing the same KO groups, I will want to drop the KO groups which are not present. If I fill in with zeros, then the model weights corresponding to those input features will not be trained. If input data which *does* contain those KO groups is used as input to the trained model, then it could adversely effect predictions.\n",
    "\n",
    "I first want to make sure that the columns in the Jablonska and Madin KO data are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of KO groups in the Madin data: 10314\n"
     ]
    }
   ],
   "source": [
    "madin_kos = list(load_data_madin('KO').columns)\n",
    "assert len(madin_kos) == len(set(madin_kos)), 'Some of the KO groups in the Madin data are duplicated.'\n",
    "print('Number of KO groups in the Madin data:', len(madin_kos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of KO groups in the Jablonska data: 9198\n"
     ]
    }
   ],
   "source": [
    "jablonska_kos = list(load_data_jablonska('KO').columns)\n",
    "assert len(jablonska_kos) == len(set(jablonska_kos)), 'Some of the KO groups in the Madin data are duplicated.'\n",
    "print('Number of KO groups in the Jablonska data:', len(jablonska_kos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of KO groups in the union: 10409\n"
     ]
    }
   ],
   "source": [
    "all_kos = set(jablonska_kos).union(set(madin_kos))\n",
    "print('Number of KO groups in the union:', len(all_kos))\n",
    "\n",
    "# Write the KO groups to a file. \n",
    "all_kos_df = pd.DataFrame({'ko':list(all_kos)}).set_index('ko')\n",
    "all_kos_df.to_csv(os.path.join(DATA_PATH, 'kos.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like a best call to use the union of the columns in each dataset. This should maximize the amount of information, while ensuring that all weights are actually updated during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terminal oxidase-related KO groups: 38\n"
     ]
    }
   ],
   "source": [
    "TERMINAL_OXIDASE_KOS = set(pd.read_csv(os.path.join(DATA_PATH, 'terminal_oxidase_kos.csv')).ko.unique())\n",
    "print('Number of terminal oxidase-related KO groups:', len(TERMINAL_OXIDASE_KOS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terminal oxidase KO groups in the Jablonska data: 24\n",
      "Number of terminal oxidase KO groups in the Madin data: 26\n"
     ]
    }
   ],
   "source": [
    "# How many of the terminal oxidase KO groups are in the Jablonska data? What about the Madin data?\n",
    "print('Number of terminal oxidase KO groups in the Jablonska data:', len(TERMINAL_OXIDASE_KOS.intersection(jablonska_kos)))\n",
    "print('Number of terminal oxidase KO groups in the Madin data:', len(TERMINAL_OXIDASE_KOS.intersection(madin_kos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I adjusted the dataset construction so that the feature set columns were the *union* of the Jablonska and Madin data, rather than the intersection. This seems to have increased the number of columns in several of the feature sets, and a marked increase in the performance of the `aa_1mer`-based classifier. Does the `aa_1mer` feature set have the number of columns we expect (21 or 20)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'E' 'Q' 'L' 'A' 'I' 'S' 'V' 'G' 'D' 'P' 'R' 'W' 'C' 'K' 'H' 'Y' 'T'\n",
      " 'F' 'N' 'J']\n",
      "Number of aa_1mer features: 21\n"
     ]
    }
   ],
   "source": [
    "print(dataset_load_feature_order('aa_1mer'))\n",
    "print('Number of aa_1mer features:', len(dataset_load_feature_order('aa_1mer')))\n",
    "\n",
    "# Why on earth are there 25?\n",
    "\n",
    "# It seems like a bad idea to include the stop symbol, as the contigs do not include the stop symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'T' 'G' 'C']\n",
      "Number of nt_1mer features: 4\n"
     ]
    }
   ],
   "source": [
    "print(dataset_load_feature_order('nt_1mer'))\n",
    "print('Number of nt_1mer features:', len(dataset_load_feature_order('nt_1mer')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'T' 'G' 'C' 'N' 'R' 'Y' 'W' 'M' 'K' 'S' 'V' 'H' 'D']\n",
      "Number of cds_1mer features: 14\n"
     ]
    }
   ],
   "source": [
    "print(dataset_load_feature_order('cds_1mer'))\n",
    "print('Number of cds_1mer features:', len(dataset_load_feature_order('cds_1mer')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    696.266968\n",
       "T    696.188965\n",
       "G    710.367249\n",
       "C    710.734253\n",
       "dtype: float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_1mer_dataset, _ = dataset_load_training_validation('nt_1mer', to_numpy=False)\n",
    "nt_1mer_dataset['features'].sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can't actually remember where the code I used to select the genomes to use is. I should make sure I am doing what I intended to do, which is get all the complete genomes from the *testing* (or validation) dataset. Note that when I add an additional split for the validation data, I will need to re-do this. \n",
    "\n",
    "Looking at the code, I think I might not be filtering for complete genomes only. I would argue that this is actually not really a problem, particularly because we are evaluating our model on some incomplete genomes. I would also argue that it's fine because we are trying to determine when the contig prediction *agrees* with the model prediction, not if the contig prediction matches the correct label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique genomes in the validation dataset: 660\n"
     ]
    }
   ],
   "source": [
    "_, validation_dataset = dataset_load_training_validation('nt_1mer', to_numpy=False)\n",
    "print('Number of unique genomes in the validation dataset:', len(validation_dataset['labels'].index.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aerobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
